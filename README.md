## ğŸ‘‹ Hi, I'm VO DAO TUAN DAT!

ğŸ“ I'm currently a student majoring in **Embedded Systems and IoT**.  
ğŸ¤– Although my major focuses on hardware, my true passion is in **Artificial Intelligence**
## ğŸŒŸ Technical Focus & Skills
ğŸŒ± **Machine Learning**
Supervised & unsupervised learning, classification, regression, model evaluation, feature engineering.

ğŸ’¡ Deep Learning
Neural networks (CNN, RNN, GRU, Transformers), model training, optimization, and transfer learning.

ğŸ‘ï¸â€ğŸ—¨ï¸ Computer Vision (OpenCV, MediaPipe, YOLO)
Image processing, object detection, pose estimation, segmentation, and real-time vision applications.

ğŸ§  NLP & LLMs (Transformers, GRU, RNN)
Text classification, sequence modeling, embeddings, attention mechanisms, and fine-tuning pre-trained LLMs.

ğŸ¤ Integrating AI with real-time applications and UI
Bridging AI models with interactive systems for smart and responsive solutions.

I'm always exploring how to bring AI into embedded systems and desktop/mobile interfaces.

---

### ğŸ”§ Tech Stack

#### AI & Machine Learning
- Python, PyTorch, TensorFlow, Keras
- OpenCV, MediaPipe, YOLOv5/YOLOv8
- Scikit-learn, HuggingFace Transformers, GRU, RNN

#### Embedded & IoT
- C/C++, ESP32, Arduino
- MQTT, Firebase
- Raspberry Pi, Sensors, Edge AI concepts

#### UI & Full-stack
- PyQt5, C# (WinForms/WPF)
- RESTful API integration (FastAPI, Flask)
- Firebase Realtime DB

#### Dev Tools
- Git/GitHub, VS Code, Linux, Jupyter, Docker

---

### ğŸš€ Featured Projects

#### ğŸ–ï¸ Control PC Functions via Hand Gestures
## Key features:
This project enables users to control various PC functions using only hand gestures captured through a webcam. Utilizing the MediaPipe framework for real-time hand landmark detection, combined with image processing and external libraries, the system allows gesture-based control for tasks such as:
- Mouse Control: Move the cursor and perform left-clicks using finger positions.
- Volume Control: Adjust the system volume by changing the distance between specific fingers.
- Screen Brightness Adjustment: Modify brightness levels through gesture patterns.
- Screenshot Capture: Automatically capture screenshots using a unique hand gesture.
- Exit Function: Exit the program with a simple gesture trigger.
## Key technologies and libraries used:
- **MediaPipe** for extracting hand landmarks.
- **OpenCV** for image processing and display.
- **AutoPy** for mouse control.
- **pycaw** for volume manipulation.
- **screen_brightness_control** for brightness settings.
- **pyautogui** for screen capturing.
---
#### âœ‹ Hand Sign Language Teaching System
## Key features:
A real-time, AI-powered learning environment to support Vietnamese hand sign language education.
ğŸ“˜ Study module: Teaches Vietnamese hand sign alphabet with interactive visuals.
ğŸ§  AI assessment: Uses computer vision and GRU-based sequence models to evaluate user gestures.
ğŸ’» Interactive UI: Built using C# and PyQt5, enabling a dynamic and user-friendly experience.
ğŸ”— Model integration: Real-time inference with YOLOv8, MediaPipe, and custom PyTorch models through FastAPI and WebSocket for low-latency communication.
## Key technologies and libraries used:
# YOLOv8, GRU, PyTorch, Python, C#, PyQt5, FastAPI, WebSocket, MediaPipe
### ğŸ“š Currently Learning
ğŸ¤– Researching AI solutions for Vietnamese Sign Language recognition
ğŸ”— Integrating AI models with user interfaces via API for seamless interaction
ğŸ§  Exploring AI Agents, Large Language Models (LLMs), and Generative AI (GenAI) for next-gen intelligent systems
---

### ğŸ“« Let's Connect!
- Email: tuandat1102004@gmail.com  
- LinkedIn: https://www.linkedin.com/in/tuandatdatd/
- Facebook: https://www.facebook.com/tuandatdatd/


Thanks for checking out my GitHub!
