## 👋 Hi, I'm VO DAO TUAN DAT!

🎓 I'm currently a student majoring in **Embedded Systems and IoT**.  
🤖 Although my major focuses on hardware, my true passion is in **Artificial Intelligence**
## 🌟 Technical Focus & Skills
🌱 **Machine Learning**
Supervised & unsupervised learning, classification, regression, model evaluation, feature engineering.

💡 Deep Learning
Neural networks (CNN, RNN, GRU, Transformers), model training, optimization, and transfer learning.

👁️‍🗨️ Computer Vision (OpenCV, MediaPipe, YOLO)
Image processing, object detection, pose estimation, segmentation, and real-time vision applications.

🧠 NLP & LLMs (Transformers, GRU, RNN)
Text classification, sequence modeling, embeddings, attention mechanisms, and fine-tuning pre-trained LLMs.

🤝 Integrating AI with real-time applications and UI
Bridging AI models with interactive systems for smart and responsive solutions.

I'm always exploring how to bring AI into embedded systems and desktop/mobile interfaces.

---

### 🔧 Tech Stack

#### AI & Machine Learning
- Python, PyTorch, TensorFlow, Keras
- OpenCV, MediaPipe, YOLOv5/YOLOv8
- Scikit-learn, HuggingFace Transformers, GRU, RNN

#### Embedded & IoT
- C/C++, ESP32, Arduino
- MQTT, Firebase
- Raspberry Pi, Sensors, Edge AI concepts

#### UI & Full-stack
- PyQt5, C# (WinForms/WPF)
- RESTful API integration (FastAPI, Flask)
- Firebase Realtime DB

#### Dev Tools
- Git/GitHub, VS Code, Linux, Jupyter, Docker

---

### 🚀 Featured Projects

#### 🖐️ Control PC Functions via Hand Gestures
## Key features:
This project enables users to control various PC functions using only hand gestures captured through a webcam. Utilizing the MediaPipe framework for real-time hand landmark detection, combined with image processing and external libraries, the system allows gesture-based control for tasks such as:
- Mouse Control: Move the cursor and perform left-clicks using finger positions.
- Volume Control: Adjust the system volume by changing the distance between specific fingers.
- Screen Brightness Adjustment: Modify brightness levels through gesture patterns.
- Screenshot Capture: Automatically capture screenshots using a unique hand gesture.
- Exit Function: Exit the program with a simple gesture trigger.
## Key technologies and libraries used:
- **MediaPipe** for extracting hand landmarks.
- **OpenCV** for image processing and display.
- **AutoPy** for mouse control.
- **pycaw** for volume manipulation.
- **screen_brightness_control** for brightness settings.
- **pyautogui** for screen capturing.
---
#### ✋ Hand Sign Language Teaching System
## Key features:
A real-time, AI-powered learning environment to support Vietnamese hand sign language education.
📘 Study module: Teaches Vietnamese hand sign alphabet with interactive visuals.
🧠 AI assessment: Uses computer vision and GRU-based sequence models to evaluate user gestures.
💻 Interactive UI: Built using C# and PyQt5, enabling a dynamic and user-friendly experience.
🔗 Model integration: Real-time inference with YOLOv8, MediaPipe, and custom PyTorch models through FastAPI and WebSocket for low-latency communication.
## Key technologies and libraries used:
# YOLOv8, GRU, PyTorch, Python, C#, PyQt5, FastAPI, WebSocket, MediaPipe
### 📚 Currently Learning
🤖 Researching AI solutions for Vietnamese Sign Language recognition
🔗 Integrating AI models with user interfaces via API for seamless interaction
🧠 Exploring AI Agents, Large Language Models (LLMs), and Generative AI (GenAI) for next-gen intelligent systems
---

### 📫 Let's Connect!
- Email: tuandat1102004@gmail.com  
- LinkedIn: https://www.linkedin.com/in/tuandatdatd/
- Facebook: https://www.facebook.com/tuandatdatd/


Thanks for checking out my GitHub!
